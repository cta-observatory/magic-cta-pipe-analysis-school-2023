{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Spectrum and light curve from DL3\n",
    "\n",
    "This notebook must be used with a gammapy version v0.20 or higher. Here we will use the following conda environment:\n",
    "\n",
    "`conda activate gammapy-v1.0`\n",
    "\n",
    "If you are running locally follow the instructions on the indico page to get the same environment: https://indico.cta-observatory.org/event/4547/\n",
    "\n",
    "In this notebook we analyze DL3 with `gammapy` and we will produce:\n",
    "1) sky map\n",
    "2) the signal excess significance (on Crab data)\n",
    "3) the measured Spectral Energy Distribution (SED)\n",
    "4) the light curve\n",
    "\n",
    "We will then fit the SED with a LogParabola model given by  \n",
    "\n",
    "## $ \\Phi =  \\Phi_0 \\left(\\frac{E}{E_{ref}}\\right)^{\\alpha + \\beta\\log_{10}(E/E_{ref})}     $\n",
    "\n",
    "where we compare it with the archival Crab spectrum, which has $\\Phi_0 = 3.23\\times 10^{-11}$ cm$^{-2}$ s$^{-1}$ TeV$^{-1}$, $E_{ref} = 1$ TeV, $\\alpha = -2.47$, and $\\beta = -0.24$.\n",
    "\n",
    "\n",
    "\n",
    "### As usual, let's start by loading some modules (and checking the `gammapy` version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.time import Time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use gammapy v0.20 or later.\n",
    "\n",
    "import gammapy\n",
    "\n",
    "print(f\"gammapy: v{gammapy.__version__}\")\n",
    "\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.datasets import Datasets, SpectrumDataset\n",
    "from gammapy.estimators import FluxPointsEstimator, LightCurveEstimator\n",
    "from gammapy.makers import (\n",
    "    ReflectedRegionsBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    "    SpectrumDatasetMaker,\n",
    "    WobbleRegionsFinder,\n",
    ")\n",
    "from gammapy.maps import Map, MapAxis, RegionGeom\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.modeling.models import (\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    LogParabolaSpectralModel,\n",
    "    PowerLawSpectralModel,\n",
    "    SkyModel,\n",
    "    create_crab_spectral_model,\n",
    ")\n",
    "from gammapy.visualization import plot_spectrum_datasets_off_regions\n",
    "from regions import CircleSkyRegion, PointSkyRegion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the pyplot figure\n",
    "plt.rcParams.update(\n",
    "    {\"figure.figsize\": (12, 9), \"font.size\": 15, \"grid.linestyle\": \"--\"}\n",
    ")\n",
    "\n",
    "# Get the default color cycle\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DL3 files\n",
    "\n",
    "Here we load the DL3 files into Gammapy data containers (i.e.: **DataStore objects**) .\n",
    "\n",
    "First we need to provide the directories where the DL3 and index files are stored.\n",
    "\n",
    "The meaning of the following directory's labels are:  \n",
    "*_wgt*: IRFs for the observation are calculated by linear interpolations of IRFs in the 3 closest test nodes. The wheigts of IRFs depends on the zenith and azimuth distance.  \n",
    "*_int*: to get the \"event\" reconstructed values (energy, gammaness, arrival direction) the individual telescope results are weighted by their Intensities.  \n",
    "*_dyn*: dynamic theta (gammaness cuts have been used as dynamic in all datasets uploaded here)  \n",
    "\n",
    "In this example, in case we are running at the IT container, we use the data here:\n",
    "`/fefs/aswg/workspace/2023_joint_analysis_school/IRF_and_DL3/input/input_step_5/DL3_wgt/`   \n",
    "`/fefs/aswg/workspace/2023_joint_analysis_school/IRF_and_DL3/input/input_step_5/DL3_int/`  \n",
    "produced with dynamic gammaness cuts, global theta cuts, IRF weighting and single telescope weighting.\n",
    "\n",
    "It is possible to run this notebook on a single dataset or both. Running on both allows to check the effect of the differnt choices. If you wish to process only one set of DL3 files, please set `input_dir_1 = \"\"` below.\n",
    "\n",
    "Let's start by creating DataStore objects from the input directories and take a look at one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input_dir = '/fefs/aswg/workspace/2023_joint_analysis_school/IRF_and_DL3/input/input_step_5/DL3_wgt/'\n",
    "#input_dir_1 = '/fefs/aswg/workspace/2023_joint_analysis_school/IRF_and_DL3/input/input_step_5/DL3_int/'\n",
    "input_dir = '/home/dipierr/data/software-school-2023/IRF_and_DL3/input/input_step_5/DL3_wgt/'\n",
    "input_dir_1 = '/home/dipierr/data/software-school-2023/IRF_and_DL3/input/input_step_5/DL3_int/'\n",
    "\n",
    "double = True\n",
    "if input_dir_1=='':\n",
    "    double = False\n",
    "\n",
    "data_store = DataStore.from_dir(input_dir)\n",
    "if double:\n",
    "    data_store_1 = DataStore.from_dir(input_dir_1)\n",
    "    \n",
    "# Show the observation table\n",
    "data_store.obs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the features of the observations (i.e. the runs).\n",
    "\n",
    "In the cell below, if we set `obs_ids = None` this means that we will process all the runs. If you want to use only some of the runs, please insert instead a list of comma-separated runs.\n",
    "\n",
    "In this step we use the Gammapy objects \"Observation\", which contain a lot of information (e.g.: events, hdu tables, meta data,...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "obs_ids = None   # `None` means \"all\" observations\n",
    "obs_ids_1 = None\n",
    "\n",
    "observations = data_store.get_observations(obs_ids, required_irf=\"point-like\")\n",
    "if double:\n",
    "    observations_1 = data_store_1.get_observations(obs_ids_1, required_irf=\"point-like\")\n",
    "\n",
    "print(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ON region\n",
    "\n",
    "Here we will collect the target position from observations metadata. To define the `on_region` we use `CircleSkyRegion` in case of global theta cuts, `PointSkyRegion` in case of dynamic theta cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata from the first observation\n",
    "observation = observations[0]\n",
    "\n",
    "event_meta = observation.events.table.meta\n",
    "aeff_meta = observation.aeff.meta\n",
    "\n",
    "# Collecting the target position\n",
    "target_position = SkyCoord(\n",
    "    u.Quantity(event_meta[\"RA_OBJ\"], u.deg),\n",
    "    u.Quantity(event_meta[\"DEC_OBJ\"], u.deg),\n",
    "    frame=\"icrs\",\n",
    ")\n",
    "\n",
    "if \"RAD_MAX\" in aeff_meta:\n",
    "    # Get the global theta cut used for creating the IRFs\n",
    "    on_region_radius = aeff_meta[\"RAD_MAX\"] * u.deg\n",
    "    \n",
    "    # Use the circle sky region to apply the global theta cut\n",
    "    on_region = CircleSkyRegion(center=target_position, radius=on_region_radius)\n",
    "    \n",
    "else:\n",
    "    # Use the point sky region to apply dynamic theta cuts\n",
    "    on_region = PointSkyRegion(target_position)\n",
    "    \n",
    "print(\"Position of the ON region: \\n\",on_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are providing a second input sample, let's collect also its position since the two datasets can have different types of theta cuts (in this case however they are identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if double:\n",
    "    # Get metadata from the first observation\n",
    "    observation_1 = observations_1[0]\n",
    "\n",
    "    event_meta_1 = observation_1.events.table.meta\n",
    "    aeff_meta_1 = observation_1.aeff.meta\n",
    "\n",
    "    # Define a target position\n",
    "    target_position_1 = SkyCoord(\n",
    "        u.Quantity(event_meta[\"RA_OBJ\"], u.deg),\n",
    "        u.Quantity(event_meta[\"DEC_OBJ\"], u.deg),\n",
    "        frame=\"icrs\",\n",
    "    )\n",
    "\n",
    "    if \"RAD_MAX\" in aeff_meta:\n",
    "        # Get the global theta cut used for creating the IRFs\n",
    "        on_region_radius = aeff_meta[\"RAD_MAX\"] * u.deg\n",
    "\n",
    "        # Use the circle sky region to apply the global theta cut\n",
    "        on_region_1 = CircleSkyRegion(center=target_position, radius=on_region_radius)\n",
    "\n",
    "    else:\n",
    "        # Use the point sky region to apply dynamic theta cuts\n",
    "        on_region_1 = PointSkyRegion(target_position_1)\n",
    "\n",
    "    print(\"Position of the second ON region: \\n\",on_region_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the data reduction chain\n",
    "\n",
    "Here we create the energy axes (reconstructed and true energy) and we set the number of OFF regions.\n",
    "\n",
    "We also create the Gammapy **Makers** needed to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# === Settings ===\n",
    "# ================\n",
    "\n",
    "energy_min = \"0.030 TeV\"\n",
    "energy_max = \"30. TeV\"\n",
    "n_bins_pdec = 5\n",
    "\n",
    "true_energy_min = \"0.01 TeV\"\n",
    "true_energy_max = \"100 TeV\"\n",
    "n_bins_pdec_true = 10\n",
    "\n",
    "n_off_regions = 3\n",
    "\n",
    "# ============\n",
    "# === Main ===\n",
    "# ============\n",
    "\n",
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    energy_min,\n",
    "    energy_max,\n",
    "    nbin=n_bins_pdec,\n",
    "    per_decade=True,\n",
    "    name=\"energy\",\n",
    ")\n",
    "\n",
    "energy_axis_true = MapAxis.from_energy_bounds(\n",
    "    true_energy_min,\n",
    "    true_energy_max,\n",
    "    nbin=n_bins_pdec_true,\n",
    "    per_decade=True,\n",
    "    name=\"energy_true\",\n",
    ")\n",
    "\n",
    "#print(\"Energy axis:\")\n",
    "#print(energy_axis.edges)\n",
    "\n",
    "on_geom = RegionGeom.create(region=on_region, axes=[energy_axis])\n",
    "\n",
    "dataset_empty = SpectrumDataset.create(geom=on_geom, energy_axis_true=energy_axis_true)\n",
    "\n",
    "# Create a spectrum dataset maker\n",
    "dataset_maker = SpectrumDatasetMaker(\n",
    "    containment_correction=False,\n",
    "    selection=[\"counts\", \"exposure\", \"edisp\"],\n",
    "    use_region_center=True,\n",
    ")\n",
    "\n",
    "# Create a background maker\n",
    "print(f\"\\nNumber of OFF regions: {n_off_regions}\")\n",
    "\n",
    "region_finder = WobbleRegionsFinder(n_off_regions=n_off_regions)\n",
    "bkg_maker = ReflectedRegionsBackgroundMaker(region_finder=region_finder)\n",
    "\n",
    "# Create a safe mask maker\n",
    "safe_mask_maker = SafeMaskMaker(methods=[\"aeff-max\"], aeff_percent=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create new makers for the second data sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if double:\n",
    "    on_geom_1 = RegionGeom.create(region=on_region_1, axes=[energy_axis])\n",
    "    dataset_empty_1 = SpectrumDataset.create(geom=on_geom_1, energy_axis_true=energy_axis_true)\n",
    "    # Create a spectrum dataset maker\n",
    "    dataset_maker_1 = SpectrumDatasetMaker(\n",
    "        containment_correction=False,\n",
    "        selection=[\"counts\", \"exposure\", \"edisp\"],\n",
    "        use_region_center=True,\n",
    "    )\n",
    "\n",
    "    # Create a background maker\n",
    "    print(f\"\\nNumber of OFF regions: {n_off_regions}\")\n",
    "\n",
    "    region_finder_1 = WobbleRegionsFinder(n_off_regions=n_off_regions)\n",
    "    bkg_maker_1 = ReflectedRegionsBackgroundMaker(region_finder=region_finder_1)\n",
    "\n",
    "    # Create a safe mask maker\n",
    "    safe_mask_maker_1 = SafeMaskMaker(methods=[\"aeff-max\"], aeff_percent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let Gammapy to make the magic...\n",
    "\n",
    "Now we apply the makers to the observations to create new datasets that contain the number of events, number of excess and background events, exposure and $\\delta = 1/R_{OFF}$, where $R_{OFF}$ is the number of OFF regions. We will use this $\\delta$ later to compute the Li & Ma significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Datasets()\n",
    "\n",
    "counts = Map.create(skydir=target_position, width=3)\n",
    "\n",
    "# Loop over every observation\n",
    "print(\"Running the makers...\")\n",
    "\n",
    "n_observations = len(observations)\n",
    "\n",
    "for i_obs, observation in enumerate(observations):\n",
    "    \n",
    "    if (i_obs % 10) == 0:\n",
    "        print(f\"{i_obs}/{n_observations}\")\n",
    "    \n",
    "    obs_id = observation.obs_id\n",
    "    \n",
    "    # Fill the number of events in the map\n",
    "    counts.fill_events(observation.events)\n",
    "\n",
    "    # Run the makers to the observation data\n",
    "    dataset = dataset_maker.run(dataset_empty.copy(name=str(obs_id)), observation)\n",
    "    dataset_on_off = bkg_maker.run(dataset, observation)\n",
    "    dataset_on_off = safe_mask_maker.run(dataset_on_off, observation)\n",
    "\n",
    "    datasets.append(dataset_on_off)\n",
    "\n",
    "print(f\"{n_observations}/{n_observations}\")\n",
    "\n",
    "# Get the information table\n",
    "info_table = datasets.info_table(cumulative=True)\n",
    "print(datasets.energy_ranges)\n",
    "\n",
    "# Show the table\n",
    "info_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again for the second sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if double:\n",
    "    datasets_1 = Datasets()\n",
    "\n",
    "    counts_1 = Map.create(skydir=target_position_1, width=3)\n",
    "\n",
    "    # Loop over every observation\n",
    "    print(\"Running the makers...\")\n",
    "\n",
    "    n_observations_1 = len(observations_1)\n",
    "\n",
    "    for i_obs, observation in enumerate(observations_1):\n",
    "\n",
    "        if (i_obs % 10) == 0:\n",
    "            print(f\"{i_obs}/{n_observations_1}\")\n",
    "\n",
    "        obs_id = observation.obs_id\n",
    "\n",
    "        # Fill the number of events in the map\n",
    "        counts_1.fill_events(observation.events)\n",
    "\n",
    "        # Run the makers to the observation data\n",
    "        dataset = dataset_maker_1.run(dataset_empty_1.copy(name=str(obs_id)), observation)\n",
    "        dataset_on_off = bkg_maker_1.run(dataset, observation)\n",
    "        dataset_on_off = safe_mask_maker_1.run(dataset_on_off, observation)\n",
    "\n",
    "        datasets_1.append(dataset_on_off)\n",
    "\n",
    "    print(f\"{n_observations_1}/{n_observations_1}\")\n",
    "\n",
    "    # Get the information table\n",
    "    info_table_1 = datasets_1.info_table(cumulative=True)\n",
    "    print(datasets_1.energy_ranges)\n",
    "\n",
    "    # Show the table\n",
    "    info_table_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sky map\n",
    "\n",
    "Let's create a counts map showing also the ON-OFF regions.\n",
    "In the legend we list the run number for each set of OFF regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Plot the count map\n",
    "ax = counts.plot(add_cbar=True)\n",
    "\n",
    "# Plot the ON position\n",
    "on_geom.plot_region(ax=ax, edgecolor=\"red\")\n",
    "\n",
    "# Plot the OFF positions (only the first part of observations)\n",
    "if n_observations < 10:\n",
    "    plot_spectrum_datasets_off_regions(datasets, ax)\n",
    "else:\n",
    "    plot_spectrum_datasets_off_regions(datasets[0:10], ax)\n",
    "\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excess significance\n",
    "\n",
    "Now let's plot the number of events and significance (i.e. sqrt(TS), in $\\sigma$ units) as a function of livetime, which is the total amount of usefull time collecting data from the target, i.e. livetime $= t_{on} - t_{dead}$.\n",
    "\n",
    "As expected, the number of excess events (and so the significance) increase faster than the background in terms of the livetime, reaching a 50 $\\sigma$ significance in less than 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 7))\n",
    "grid = (1, 2)\n",
    "\n",
    "# Plot the number of events along the livetime\n",
    "plt.subplot2grid(grid, (0, 0))\n",
    "plt.xlabel(\"Livetime [hour]\")\n",
    "plt.ylabel(\"Number of events\")\n",
    "\n",
    "plt.plot(\n",
    "    info_table[\"livetime\"].to(\"h\"),\n",
    "    info_table[\"excess\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Excess\",\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    info_table[\"livetime\"].to(\"h\"),\n",
    "    info_table[\"background\"],\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Background\",\n",
    ")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# Plot the significance along the livetime\n",
    "plt.subplot2grid(grid, (0, 1))\n",
    "plt.xlabel(\"Livetime [hour]\")\n",
    "plt.ylabel(\"Sqrt(TS)\")\n",
    "\n",
    "plt.plot(\n",
    "    info_table[\"livetime\"].to(\"h\"), info_table[\"sqrt_ts\"], marker=\"o\", linestyle=\"--\"\n",
    ")\n",
    "\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the spectrum\n",
    "\n",
    "We now fit the LogParabola model defined in the top of this notebook to our data. We start by creating a `SkyModel` and adding it to our datasets, we create a `Fit` that we will run on our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name=\"Crab Nebula\"\n",
    "\n",
    "spectral_model = LogParabolaSpectralModel(\n",
    "    amplitude=u.Quantity(5e-12, unit=\"cm-2 s-1 TeV-1\"),\n",
    "    alpha=2,\n",
    "    beta=0.1,\n",
    "    reference=u.Quantity(1, unit=\"TeV\"),\n",
    ")\n",
    "\n",
    "sky_model = SkyModel(spectral_model=spectral_model.copy(), name=source_name)\n",
    "\n",
    "# Add the model to the stacked dataset\n",
    "stacked_dataset = datasets.stack_reduce()\n",
    "stacked_dataset.models = [sky_model]\n",
    "\n",
    "if double:    \n",
    "    stacked_dataset_1 = datasets_1.stack_reduce()\n",
    "    stacked_dataset_1.models = [sky_model]\n",
    "\n",
    "# Create a fit object to run on the datasets\n",
    "fit = Fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the `fit` on the first sample to get the LogParabola parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = fit.run(datasets=stacked_dataset)\n",
    "print(results)\n",
    "\n",
    "# Keep the best fit model\n",
    "best_fit_model = stacked_dataset.models[0].spectral_model.copy()\n",
    "\n",
    "# Show the fitted parameters\n",
    "stacked_dataset.models.to_parameters_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the second sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if double:    \n",
    "    results_1 = fit.run(datasets=stacked_dataset_1)\n",
    "    print(results_1)\n",
    "\n",
    "    # Keep the best fit model\n",
    "    best_fit_model_1 = stacked_dataset_1.models[0].spectral_model.copy()\n",
    "\n",
    "    # Show the fitted parameters\n",
    "    stacked_dataset_1.models.to_parameters_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if the observations are in good agreement with the predictions. We do it by plotting the number of excess events and comparing it to the number of predicted events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Plot the number of excess and predicted events\n",
    "ax_spectrum, ax_residuals = stacked_dataset.plot_fit()\n",
    "\n",
    "ax_spectrum.set_ylabel(\"Number of events\")\n",
    "ax_spectrum.grid()\n",
    "\n",
    "plt.setp(ax_spectrum.get_xticklabels(), visible=False)\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "ax_residuals.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if double:\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot the number of excess and predicted events\n",
    "    ax_spectrum, ax_residuals = stacked_dataset_1.plot_fit()\n",
    "\n",
    "    ax_spectrum.set_ylabel(\"Number of events\")\n",
    "    ax_spectrum.grid()\n",
    "\n",
    "    plt.setp(ax_spectrum.get_xticklabels(), visible=False)\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "\n",
    "    ax_residuals.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the flux points\n",
    "\n",
    "We create a `FluxPointsEstimator` object to be applied on our datasets in order to evaluate the SEDs (`sed_type=\"e2dnde\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a flux point estimator\n",
    "flux_points_estimator = FluxPointsEstimator(\n",
    "    energy_edges=energy_axis.edges, source=source_name, selection_optional=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first data sample we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the flux point estimator to the datasets\n",
    "print(\"Running the flux points estimator...\")\n",
    "flux_points = flux_points_estimator.run(datasets=stacked_dataset)\n",
    "\n",
    "# Show the flux points table\n",
    "flux_points.to_table(sed_type=\"e2dnde\", formatted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While for the second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if double:\n",
    "    # Run the flux point estimator to the datasets\n",
    "    print(\"Running the flux points estimator...\")\n",
    "    flux_points_1 = flux_points_estimator.run(datasets=stacked_dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_points_1.to_table(sed_type=\"e2dnde\", formatted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the SED\n",
    "\n",
    "Below we show the SEDs for the two datasets in the same plot, showing the flux points and best fit models. For comparison, we plot the Crab reference SED (MAGIC, JHEAp 2015)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ================\n",
    "# === Settings ===\n",
    "# ================\n",
    "\n",
    "sed_type = \"e2dnde\"\n",
    "yunits = u.Unit(\"erg cm-2 s-1\")\n",
    "\n",
    "crab_model = create_crab_spectral_model(\"magic_lp\")\n",
    "#print(crab_model)\n",
    "reference_models = {\n",
    "    \"Crab reference (MAGIC, JHEAp 2015)\": crab_model,\n",
    "    \n",
    "}\n",
    "\n",
    "# ============\n",
    "# === Main ===\n",
    "# ============\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "energy_bounds = energy_axis.edges[[0, -1]]\n",
    "\n",
    "lines = itertools.cycle([\"--\", \"-.\", \":\"])\n",
    "\n",
    "# Plot the flux points\n",
    "ax = flux_points.plot(sed_type=sed_type, label=\"LST-1 + MAGIC (this tutorial), first dataset\")\n",
    "if double:\n",
    "    ax = flux_points_1.plot(sed_type=sed_type, label=\"LST-1 + MAGIC (this tutorial), second dataset\")\n",
    "\n",
    "\n",
    "# Plot the best fit model and its error, first sample\n",
    "best_fit_model.plot(\n",
    "    ax=ax,\n",
    "    energy_bounds=energy_bounds,\n",
    "    sed_type=sed_type,\n",
    "    yunits=yunits,\n",
    "    color=colors[0],\n",
    "    label=\"Best fit, first dataset\"\n",
    ")\n",
    "\n",
    "best_fit_model.plot_error(\n",
    "    ax=ax,\n",
    "    energy_bounds=energy_bounds,\n",
    "    sed_type=sed_type,\n",
    "    yunits=yunits,\n",
    "    facecolor=colors[0],\n",
    ")\n",
    "\n",
    "if double:\n",
    "    # Plot the best fit model and its error, second sample\n",
    "    best_fit_model_1.plot(\n",
    "        ax=ax,\n",
    "        energy_bounds=energy_bounds,\n",
    "        sed_type=sed_type,\n",
    "        yunits=yunits,\n",
    "        color=colors[1],\n",
    "        label=\"Best fit, second dataset \"\n",
    "    )\n",
    "\n",
    "    best_fit_model_1.plot_error(\n",
    "        ax=ax,\n",
    "        energy_bounds=energy_bounds,\n",
    "        sed_type=sed_type,\n",
    "        yunits=yunits,\n",
    "        facecolor=colors[1],\n",
    "    )\n",
    "\n",
    "\n",
    "# Plot the reference spectra\n",
    "for label, model in reference_models.items():\n",
    "\n",
    "    model.plot(\n",
    "        ax=ax,\n",
    "        energy_bounds=energy_bounds,\n",
    "        sed_type=sed_type,\n",
    "        yunits=yunits,\n",
    "        label=label,\n",
    "        linestyle=next(lines)\n",
    "    )\n",
    "\n",
    "ax.set_title(f\"Spectral Energy Distribution of the {source_name}\")\n",
    "\n",
    "ax.set_ylim(1e-12)\n",
    "ax.set_ylabel(\"$E^2dN/dE$ [erg cm$^{-2}$ s$^{-1}$]\")\n",
    "ax.grid(which=\"both\")\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the light curve\n",
    "\n",
    "For the light curve, we assume that the spectral shape parameters, $\\alpha$ and $\\beta$, do not change over the different time bins, i.e. we freeze them at their best-fit values achieved above.\n",
    "\n",
    "We start with the first data sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frozen_params = [\"alpha\", \"beta\"]\n",
    "\n",
    "sky_model = SkyModel(\n",
    "    spectral_model=best_fit_model.copy(), name=source_name\n",
    ")\n",
    "\n",
    "# Freeze the spectral parameters\n",
    "for param in frozen_params:\n",
    "    sky_model.parameters[param].frozen = True\n",
    "    \n",
    "# Add the model to the datasets\n",
    "datasets.models = [sky_model]\n",
    "\n",
    "print(sky_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do the same for the second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if double:\n",
    "    sky_model_1 = SkyModel(\n",
    "        spectral_model=best_fit_model_1.copy(), name=source_name\n",
    "    )\n",
    "\n",
    "    # Freeze the spectral parameters\n",
    "    for param in frozen_params:\n",
    "        sky_model_1.parameters[param].frozen = True\n",
    "\n",
    "    # Add the model to the datasets\n",
    "    datasets_1.models = [sky_model_1]\n",
    "\n",
    "    print(sky_model_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a `LightCurveEstimator`.\n",
    "\n",
    "We must set energy edges and time intervals to be used to compute the light curve. If we adopt `time_intervals=None` we will get a run-wise LC, if instead we give it a list of Astropy Time objects, we can create e.g. a daily binned LC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_edges = energy_axis.edges[[1,-1]]\n",
    "\n",
    "#time_intervals = [\n",
    "#    Time([59171.98, 59172.01], format=\"mjd\", scale=\"utc\"),\n",
    "#    Time([59190.98, 59190.99], format=\"mjd\", scale=\"utc\"),\n",
    "#    Time([59198.89, 59198.94], format=\"mjd\", scale=\"utc\"),\n",
    "#    Time([59258.91, 59258.99], format=\"mjd\", scale=\"utc\"),\n",
    "#    Time([59288.91, 59288.94], format=\"mjd\", scale=\"utc\"),\n",
    "#    Time([59290.94, 59290.96], format=\"mjd\", scale=\"utc\"),\n",
    "#]   # e.g. daily light curve \n",
    "\n",
    "time_intervals=None# `None` automatically makes a \"run-wise\" LC.\n",
    "\n",
    "# ============\n",
    "# === Main ===\n",
    "# ============\n",
    "\n",
    "# Create a light curve estimator\n",
    "light_curve_estimator = LightCurveEstimator(\n",
    "    energy_edges=energy_edges,\n",
    "    time_intervals=time_intervals,\n",
    "    source=source_name,\n",
    "    selection_optional=\"all\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run this estimator on the first data sample (`sed_type=\"flux\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the light curve estimator to the datasets\n",
    "print(\"Running the light curve estimator...\")\n",
    "light_curve = light_curve_estimator.run(datasets=datasets)\n",
    "\n",
    "# Show the light curve table\n",
    "light_curve.to_table(sed_type=\"flux\", format=\"lightcurve\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the second data sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if double:\n",
    "    # Run the light curve estimator to the datasets\n",
    "    print(\"Running the light curve estimator...\")\n",
    "    light_curve_1 = light_curve_estimator.run(datasets=datasets_1)\n",
    "\n",
    "    # Show the light curve table\n",
    "    light_curve_1.to_table(sed_type=\"flux\", format=\"lightcurve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LC plot\n",
    "\n",
    "Finally, we plot the LCs and compare them to the Crab reference flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "lines = itertools.cycle([\"--\", \"-.\", \":\"])\n",
    "\n",
    "# Plot the light curve\n",
    "ax = light_curve.plot(sed_type=\"flux\", label=\"LST-1 + MAGIC (this work), first set\")\n",
    "if double:\n",
    "    ax = light_curve_1.plot(sed_type=\"flux\", label=\"LST-1 + MAGIC (this work), second set\")\n",
    "\n",
    "\n",
    "xlim = plt.xlim()\n",
    "\n",
    "# Plot the reference flux\n",
    "for label, model in reference_models.items():\n",
    "    \n",
    "    integ_flux = model.integral(energy_edges[0], energy_edges[1])\n",
    "    ax.plot(xlim, np.repeat(integ_flux, 2), label=label, linestyle=next(lines))\n",
    "\n",
    "energy_range = f\"{energy_edges[0]:.3f} < $E$ < {energy_edges[1]:.1f}\"\n",
    "\n",
    "ax.set_title(f\"Light curve of {source_name} ({energy_range})\")\n",
    "ax.set_ylabel(\"Flux [cm$^{-2}$ s$^{-1}$]\")\n",
    "ax.set_xlabel(\"Time [date-hour]\")\n",
    "ax.set_yscale(\"linear\")\n",
    "ax.legend()\n",
    "ax.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
